{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "6-MemoryRetention.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "i01lxd8-CjQb"
      },
      "source": [
        "%%capture\r\n",
        "! pip install arviz==0.11.00\r\n",
        "! pip install pymc3==3.10.0"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bjv7y20nDvVx"
      },
      "source": [
        "import arviz as az\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "import pymc3 as pm\r\n",
        "import seaborn as sns\r\n",
        "from pymc3 import math\r\n",
        "\r\n",
        "from theano import tensor as tt\r\n",
        "from scipy import stats"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CDeKJoidD4g9"
      },
      "source": [
        "# Chapter 10 - Memory retention\r\n",
        "  \r\n",
        "This Chapter is about estimating the relationship between memory retention and time.\r\n",
        "The model being considered is a simplified version of the exponential decay model. The model assumes that the probability that an item will be remembered after a period of time $t$ has elapsed is $\\theta_{t} = \\text{exp}(−\\alpha t)+\\beta$, with the restriction $0 < \\theta_{t} < 1$. The $\\alpha$ parameter corresponds to the rate of decay of information. The $\\beta$ parameter corresponds to a baseline level of remembering that is assumed to remain even after very long time periods.\r\n",
        "  \r\n",
        "## 10.1 No individual differences\r\n",
        "\r\n",
        "\r\n",
        "$$ \\alpha \\sim \\text{Beta}(1,1)$$\r\n",
        "$$ \\beta \\sim \\text{Beta}(1,1)$$\r\n",
        "$$ \\theta_{j} = \\text{min}(1,\\text{exp}(−\\alpha t_{j})+\\beta)$$\r\n",
        "$$ k_{ij} \\sim \\text{Binomial}(\\theta_{j},n)$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "14-a_ynYEiti"
      },
      "source": [
        "The above model is very sensitive to the starting value. We can specify a starting value for each parameter by assigning a `testval` when the RV is created:\r\n",
        "```python\r\n",
        "alpha = pm.Beta('alpha', alpha=1, beta=1, testval=.30)\r\n",
        "```\r\n",
        "\r\n",
        "In fact, with a bad starting value, NUTS really has a hard time sampling, and we get a `Bad initial energy` error. The reason is that bounding the theta gives 0 gradient, which is a problem as NUTS needs th gradient to work."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7-9OPLWeD5YR"
      },
      "source": [
        "t = np.array([1, 2, 4, 7, 12, 21, 35, 59, 99, 200])\r\n",
        "nt = len(t)\r\n",
        "# slist = [0,1,2,3]\r\n",
        "ns = 4\r\n",
        "tmat = np.repeat(t, ns).reshape(nt, -1).T\r\n",
        "k1 = np.ma.masked_values([18, 18, 16, 13, 9, 6, 4, 4, 4, -999,\r\n",
        "                          17, 13,  9,  6, 4, 4, 4, 4, 4, -999,\r\n",
        "                          14, 10,  6,  4, 4, 4, 4, 4, 4, -999,\r\n",
        "                          -999, -999, -999, -999, -999, -999, -999, -999, -999, -999], \r\n",
        "                          value=-999).reshape(ns,-1)\r\n",
        "n = 18"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2n4_KCFYE7bq"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}